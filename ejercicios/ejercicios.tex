\begin{ejercicio}[Ejercicio 1.1]
	Sea $X$ una variable aleatoria normal centrada de varianza 1. 
	Utilice el teorema de cambio de variable para calcular
	\begin{align}
		\E{X^{2n}}
	\end{align}
	para toda $n\in\na$.
\end{ejercicio}

Primera forma:
Utilizar este resultado: \href{http://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Moments}{momentos y funci\'on caracter\'istica},

Se puede llegar f\'acilmente a que 
\begin{align}
	\E{X^{2n}} = \frac{(2n)!}{2^m m!}
\end{align}\pn

Segunda forma:
Integrar $x^2 \frac{1}{\sqrt{2 \pi} e^{-x^2 / 2}}$ y utilizar el cambio de variable $u = x^2/2$.


\pn---------------------------------------------------\pn
\begin{ejercicio}[Ejercicio 1.2]
Si $\paren{X,Y}$ son dos variables aleatorias con densidad
conjunta $f\paren{x,y}$, pruebe que:\begin{esn}
\espc{g\paren{Y}}{X}=\frac{\int f\paren{X,y}g\paren{y}\, dy}{\int {f\paren{X,y}\, dy}}.\end{esn}
\end{ejercicio}


\pn---------------------------------------------------\pn
\begin{ejercicio}[Ejercicio 1.3	]
Sean $X_1,X_2,\ldots$ vaiids. Sea $K$ una variable aleatoria independiente de $X_1,X_2,\ldots$ y con valores en $\na$. Cacule\begin{esn}
\espc{X_1+\cdots+X_K}{K}.
\end{esn}Sugerencia: ?`Qu\'e pasa cuando $K$ toma s\'olo un valor?
\end{ejercicio}
La respuesta es:
\begin{align}
	K\E(X_1)
\end{align}


\pn---------------------------------------------------\pn
Sea $U$ una variable aleatoria uniforme en $(0,1)$ y definamos a\begin{esn}
X_n=2^n\indi{U\leq 1/2^n}.
\end{esn}Entonces $X_0,X_1,\ldots$ es una martingala respecto de la filtraci\'on que genera. 
\begin{ejercicio}
Probar la afirmaci\'on anterior.
\end{ejercicio}


Consideremos el siguiente experimento aleatorio, se tiene una urna 
con $r$ bolas rojas y $v$ bolas verdes. Extraemos una bola, la 
reemplazamos junto con $c$ bolas del mismo color, revolvemos la 
urna y volvemos a realizar el experimento. Sea $X_0$ la fracci\'on 
inicial de bolas rojas en la urna  y $X_n$ la fracci\'on de bolas 
rojas  en la urna una vez realizado el experimento $n$ veces. 
Entonces $\paren{X_n}_{n\in\na}$ es una martingala con respecto a 
la filtraci\'on que genera esta sucesi\'on. Antes de proceder a verificar  
la afirmaci\'on anterior, debemos considerar el modelo matem\'atico preciso 
del experimento aleatorio en cuesti\'on, para poder calcular las esperanzas 
condicionales. 
Notemos que al momento de la $n$-\'esima extracci\'on hay
\begin{esn}
b_n=r+v+nc
\end{esn}
bolas en la urna. Sean $\paren{U_i}$ variables aleatorias independientes e id\'enticamente distribuidas, 
$r,v>0$ y definamos $X_0=r/(r+v)$ y para $n\geq 0$:
\begin{esn}
Y_{n+1}=\indi{U_{n+1}\leq X_n}\quad\text{y}\quad X_{n+1}=\frac{r+v+nc}{r+v+\paren{n+1}c}X_n+\frac{c}{r+v+\paren{n+1}c}Y_n.
\end{esn}
Esta es la descripci\'on matem\'atica que utilizaremos del experimento considerado anteriormente y en \'el, 
la variable $X_n$ es funci\'on  de $X_0,U_1,\ldots, U_n$ para $n\geq 1$ (de hecho es funci\'on de $X_{n-1}$ y $U_n$) y 
por lo tanto, $U_{n+1}$ es independiente de $\F_n$, la \sa\ generada por $X_0,\ldots, X_n$.
\begin{ejercicio}
Verificar que la sucesi\'on $X$ es una martingala respecto de $\paren{\F_n}$. 
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{ejercicio}[Descomposici\'on de Doob para submartingalas]
Sea \(X=\paren{X_n}_{n\in\na}\) una submartingala. Pruebe que \(X\) se puede descomponer de manera \'unica 
como \(X=M+A\) donde \(M\) es una martingala y \(A\) es un proceso previsible con \(A_0=0\). (Decimos que 
$A$ es previsible si $A_{n}$ es $\F_{n-1}$-medible para toda $n\geq 1$ y $A_0$ es $\F_0$-medible. Sugerencia: 
Asuma que ya tiene la descomposici\'on y calcule esperanza condicional de \(X_{n+1}\) dada \(X_n\).
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{ejercicio}
Sean \(X\) y \(Y\) dos martingalas (respecto de la misma filtraci\'on) y tales que 
\(\esp{X_i},\esp{Y_i}<\infty\) para toda \(i\). Pruebe la siguiente f\'ormula de integraci\'on 
por partes:
\begin{esn} 
\esp{X_nY_n}-\esp{X_0Y_0}=\sum_{i=1}^n \esp{\paren{X_i-X_{i-1}}\paren{Y_i-Y_{i-1}}} .
\end{esn}
\end{ejercicio}


\pn---------------------------------------------------\pn
\begin{ejercicio}[Ejercicio 1.8 p12]
Sea $X$ una supermartingala. Pruebe que si $T$ es un tiempo de paro acotado por $N$ entonces\begin{esn}
\esp{X_T}\geq \esp{X_N}.
\end{esn}
\end{ejercicio}


\pn---------------------------------------------------\pn
\begin{definicion}
Sea $C=\paren{C_n,n\geq 1}$ un proceso estoc\'astico. Decimos que $C$ es predecible respecto de $\paren{\F_n}$ si $C_n$ es $\F_{n-1}$-medible.
\end{definicion}
Si $C$ es un proceso predecible y acotado y $M$ es una martingala, formemos al nuevo proceso $C\cdot M$ como sigue:\begin{esn}
\paren{C\cdot M}_0=0\quad\text{y}\quad \paren{C\cdot M}_n=\sum_{i\leq n} C_{i}\paren{M_i-M_{i-1}}. 
\end{esn}
\begin{ejercicio}
Mostrar cuidadosamente que $C\cdot M$ es una martingala. Obtenga un enunciado an\'alogo si $M$ es una submartingala. 
\end{ejercicio}


\pn---------------------------------------------------\pn
\begin{ejercicio}[Extensiones del teorema de paro opcional]
Sea \(M=\paren{M_n,n\in\na}\) una (super)martingala respecto de una filtraci\'on \(\paren{\F_n,n\in\na}\) y sean \(S\) y \(T\) tiempos de paro.
\begin{enumerate}
		\item Pruebe que \(S\wedge T\), \(S+T\) y \(S\vee T\) son tiempos de paro.
		\item Sea \begin{esn}\F_T=\set{A\in\F:A\cap\set{T\leq n}\in\F_n\text{ para toda } n}\end{esn}es una \(\sigma\)-\'algebra, a la que nos referimos como la \(\sigma\)-\'algebra detenida en \(\tau\). Comente qu\'e puede fallar si \(T\) no es tiempo de paro. Pruebe que \(T\) es \(\F_T\)-medible. 
		\item Pruebe que si \(T\) es finito, entonces \(M_T\) es \(\F_T\)-medible.
		\item Pruebe que si \(S\leq T\leq n\) entonces \(\F_S\subset\F_T\). Si adem\'as \(T\) es acotado entonces \(X_S,X_T\in L_1\) y \begin{esn}\espc{M_T}{\F_S}\leq M_S.\end{esn}
		\item Si \(X=\paren{X_n,n\in\na}\) es un proceso estoc\'astico \(\paren{\F_n}\)-adaptado y tal que \(X_n\in L_1\) y tal que para cualesquiera tiempos de paro acotados \(S\) y \(T\) se tiene que \(\esp{X_S}=\esp{X_T}\) entonces \(X\) es una martingala. Sugerencia: considere tiempos de paro de la forma \(n\indi{A}+(n+1)\indi{A^c}\) con \(A\in\F_n\).
\end{enumerate}
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{ejercicio}
Suponga que $p>1-p$. 
\begin{enumerate}
\item Sea $\imf{\phi}{x}=\paren{p/q}^x$ y pruebe que $\paren{\imf{\phi}{S_n}}_{n\in\na}$ es martingala respecto a la filtraci\'on que genera.
\item Note que al aplicar el teorema de muestreo opcional de Doob al tiempo de paro acotado $T_{-a}\wedge T_b\wedge n$ se obtiene\begin{esn}
1=\esp{\imf{\phi}{S_{T_{-a}\wedge T_b\wedge n}}}.
\end{esn}Utilice alguna propiedad de la esperanza para pasar al l\'imite conforme $n\to\infty$ y concluir que\begin{esn}
1=\esp{\imf{\phi}{S_{T_{-a}\wedge T_b}}}=\imf{\phi}{-a}\proba{T_{-a}<T_b}+\imf{\phi}{b}\proba{T_b<T_{-a}}. 
\end{esn}Concluya con el c\'alculo expl\'icito de $\proba{T_b<T_{-a}}$.
\item Pruebe que $\paren{S_n-n\paren{2p-1}}_{n\in\na}$ es una martingala.
\item Note que al aplicar muestreo opcional al tiempo de paro $T_{-a}\wedge T_b\wedge n$ se obtiene\begin{esn}
\esp{S_{T_{-a}\wedge T_b\wedge n}}=\paren{2p-1}\esp{T_{-a}\wedge T_b\wedge n}.
\end{esn}Aplique propiedades de la esperenza al lado derecho y de la probabilidad al lado derecho que permitan pasar al l\'imite conforme $n\to\infty$ en la expresi\'on anterior y obtener:
\begin{align*}
\esp{T_{-a}\wedge T_b}&=\frac{1}{2p-1}\esp{S_{T_{-a}\wedge T_b}}
\\&=\frac{1}{2p-1}\paren{-a\proba{T_{-a}<T_b}+b\proba{T_b<T_{-a}}}
\end{align*}y calcule expl\'icitamente $\esp{T_{-a}\wedge T_b}$.
\end{enumerate}
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{teorema}
Sea $M$ una (sub)martingala y $C$ un proceso predecible y acotado entonces $C\cdot M$ es una (sub)martingala.
\end{teorema}
\begin{ejercicio}
Pruebe el teorema anterior. 
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{ejercicio}
Sea $U_n$ la cantidad de cruces hacia arriba que hace el proceso $M$ en el intervalo $[a,b]$ antes de $n$. Argumente que\begin{esn}
Y_n\geq \paren{b-a}U_n+\paren{M_n-a}^-.
\end{esn}Al tomar esperanzas verifique que se satisface la desigualdad de cruces de Doob\begin{esn}
\esp{U_n}\leq \frac{1}{b-a}\esp{\paren{a-M_n}^+}.
\end{esn}
\end{ejercicio}

\pn---------------------------------------------------\pn
\begin{proposicion}
Las variables aleatorias $\paren{Y_n}$ son intercambiables. De hecho, si $i_1,\ldots,i_n\in\set{0,1}$ y $s_n=i_1+\cdots+i_n$ entonces
\begin{align*}
\proba{Y_1=i_1,\ldots, Y_n=i_n}
&=\frac{\fa{\paren{r/c}}{s_n}\fa{\paren{v/c}}{n-s_n}}{\fa{\paren{\paren{r+v}/c}}{n}}.
%\\&=\frac{\frac{\imf{\Gamma}{r/c+s_n}\imf{\Gamma}{v/c+n-s_n}}{\imf{\Gamma}{r/c}\imf{\Gamma}{v/c}}}{\frac{\imf{\Gamma}{\paren{r+v}/c+n}}{\imf{\Gamma}{\paren{r+v}/c}}}
%\\&=\frac{\imf{\Beta}{r/c+n,v/c}}{\imf{\Beta}{r/c,v.c}}.
\end{align*}
\end{proposicion}
\begin{ejercicio}
Pruebe la proposici\'on anterior. Sugerencia, utilice el principio de inducci\'on. 
\end{ejercicio}

\pn---------------------------------------------------\pn
Una cadena de Markov es un proceso estoc\'astico de un tipo especial; se encuentra caracterizado por satisfacer la propiedad de Markov. Una manera coloquial de expresar la propiedad de Markov es que el futuro del proceso es independiente del pasado cuando se conoce el presente. Esto se puede traducir matem\'aticamente mediante el concepto de independencia condicional.
\begin{definicion}
%Sea $\ofp$ un espacio de probabilidad y $\mc{H}_1,\mc{H}_2,\G$ sub\sa s de \F. Decimos que $\mc{H}_1$ y $\mc{H}_2$ son condicionalmente independientes dada $\G$, denotado por $\condind{\mc{H}_1}{\mc{H}_2}{\G}$, si para cualesquiera variables aleatorias acotadas $H_1$ y $H_2$ tal que $H_i$ es $\mc{H}_i$-medible se tiene que
\begin{esn}
\espc{H_1H_2}{\G}=\espc{H_1}{\G}\espc{H_2}{\G}.
\end{esn}
\end{definicion}
Si $X=\paren{X_n}_{n\in\na}$ es un proceso estoc\'astico, $\paren{\F_n}_{n\in\na}$ es su filtraci\'on can\'onica asociada y\begin{esn}
\F^n=\sag{X_{n},X_{n+1},\ldots},
\end{esn}diremos que $X$ satisface la propiedad de Markov (inhomog\'enea) si $\F_n$ y $\F^n$ son condicionalmente independientes dada $X_n$. 
A trav\'es de una equivalencia del concepto de independencia condicional, podremos obtener la expresi\'on usual de la propiedad de Markov.
\begin{proposicion}
Las sub\sa s $\mc{H}_1$ y $\mc{H}_2$ son condicionalmente independientes dada $\G$ si y s\'olo si para cualquier variable aleatoria $H_1$ que sea $\mc{H}_1$-medible y acotada:
\begin{esn}
\espc{H_1}{\G,\mc{H}_1}=\espc{H_1}{\G}.
\end{esn}
\end{proposicion}
\begin{ejercicio}
Probar la proposici\'on anterior.
\end{ejercicio}

\pn---------------------------------------------------\pn
Tiempos de arribo para la caminata aleatoria simple unidimensional
Sea $\paren{\p_k,k\in\z}$ la familia Markoviana asociada a la caminata aleatoria simple con matriz de transici\'on $P_{i,i+1}=1-P_{i,i-1}=p\in (0,1)$. Sea $T_0$ el primer arribo a cero dado por\begin{esn}
T_0=\min\set{n:X_n=0}.
\end{esn}Nuestro objetivo ser\'a determinar a\begin{esn}
\imf{\phi}{s}=\imf{\se_1}{s^{T_0}}.
\end{esn}

Comenzando en $2$, la caminata aleatoria simple debe pasar por uno para llegar a cero, y la trayectoria de $2$ a $1$ tiene la misma distribuci\'on que la de $1$ a cero. Por lo tanto:\begin{esn}
\imf{\se_2}{s^{T_0}}=\imf{\phi}{s}^2.
\end{esn}Por otra parte, la propiedad de Markov al instante $1$ nos dice que\begin{esn}
\imf{\phi}{s}=\imf{\se_1}{s^{T_0}}=(1-p)s+ps\imf{\se_2}{s^{T_0}}=\paren{1-p}s+ps\imf{\phi}{s}^2.
\end{esn}As\'i, puesto que $\imf{\phi}{s}\in (0,1)$ para $s\in (0,1)$, vemos que\begin{esn}
\imf{\phi}{s}=\frac{1-\sqrt{1-4p\paren{1-p}s^2}}{2ps}.
\end{esn}Esto tiene varias implicaciones. La primera es que\begin{esn}
\imf{\p_1}{T_0<\infty}=\lim_{s\to 1}\imf{\se_1}{s^{T_0}}=\frac{1-\abs{1-2p}}{2p}
=\begin{cases}
1&p<1/2\\
\frac{q}{p}& p\geq 1/2
\end{cases}.
\end{esn}La segunda es el c\'alculo, para $p\leq 1/2$ de la esperanza de $T_0$: al derivar la ecuaci\'on que satisface $\phi$ vemos que\begin{esn}
0=1-\paren{2p-1}\imf{\phi'}{1-}
\end{esn}

La segunda consecuencia es la determinaci\'on expl\'icita de la distribuci\'on de $T_0$ bajo $\p_1$.
\begin{ejercicio}
Haga un desarrollo en serie de $\phi$ para obtener el valor exacto de $\imf{\p_1}{T_0=2n+1}$. \emph{Sugerencia: } La serie bin\'omica ser\'a de utilidad. Pruebe que\begin{esn}
\proba{T=2n+1}=\frac{1}{2n+1}\imf{\p_1}{S_{2n+1}=-1}.
\end{esn}
\end{ejercicio}
\pn---------------------------------------------------\pn
\begin{ejercicio}
Pruebe que $T_{N_t+1}-t,S_{N_t+2},S_{N_t+3},\ldots$ son variables aleatorias independientes y que $S_{N_t+2},S_{N_t+3},\ldots$ tienen la misma distribuci\'on que $S_1$. 

Pruebe que si $S_1$ tiene distribuci\'on exponencial entonces $T_{N_t+1}-t$ tambi\'en tiene distribuci\'on exponencial y por lo tanto los procesos $N$ y $N^t$ dado por $N^t_s=N_{t+s}-N_t$ tienen la misma distribuci\'on. 
\end{ejercicio}
\pn---------------------------------------------------\pn
Recordemos que el proceso de Poisson de intensidad $\lambda$ es estacionario en el siguiente sentido: el proceso $N^t$ dado por  $N^t_s=N_{t+s}-N_t,s\geq 0$ es tambi\'en un proceso de Poisson de intensidad $\lambda$. A\'un m\'as es cierto: de hecho $N^t$ es independiente de $\F^N_t=\sag{N_s:s\leq t}$. Antes de ver este resultado pasaremos por uno m\'as sencillo.
\begin{definicion}
Un proceso estoc\'astico $X=\paren{X_t,t\geq 0}$ tiene \defin{incrementos independientes} si cuando $0=t_0<t_0<\cdots<t_n$ se tiene que $X_{t_1}-X_{t_0},\ldots, X_{t_{n}}-X_{t_{n-1}}$ son variables aleatorias independientes.

Un proceso estoc\'astico $X=\paren{X_t,t\geq 0}$ tiene \defin{incrementos estacionarios} si $X_{t+s}-X_t$ tiene la misma distribuci\'on que $X_s$. 

Un \defin{proceso de L\'evy} es un proceso estoc\'astico $X$ con trayectorias \cadlag\ que comienza en cero y tiene incrementos independientes y estacionarios.
\end{definicion}
En otras palabras, un proceso de L\'evy es la versi\'on a tiempo continuo de una caminata aleatoria. 
\begin{ejercicio}
\label{IncrementosIndependientesImplicanIncrementosIndepDelPasadoEjercicio}
Pruebe que si $X$ tiene incrementos independientes entonces el proceso $X^t$ dado por $X^t_s=X_{t+s}-X_t$ es independiente de $\F^X_t=\sag{X_s:s\geq 0}$.
\end{ejercicio}
\pn---------------------------------------------------\pn
\begin{ejercicio}
Calcular la esperanza y varianza del proceso de Poisson y de Poisson compuesto (en t\'erminos de la intensidad y la distribuci\'on de salto). Probar que si $X$ es\begin{esn}
\esp{e^{iu Z_t}}=e^{-\lambda t\paren{1-\imf{\psi}{u}}}\quad\text{donde}\quad \imf{\psi}{u}=\esp{e^{iu \xi_1}}. 
\end{esn}
\end{ejercicio}
\pn---------------------------------------------------\pn
\begin{ejercicio}
Sea $N$ un proceso de L\'evy tal que $N_t$ tiene distribuci\'on de par\'ametro $\lambda t$. 
\begin{enumerate}
\item Pruebe que casi seguramente las trayectorias de $N$ son no-decrecientes.
\item Sea $\Xi$ la \'unica medida en $\mc{B}_{\re_+}$ tal que $\imf{\Xi}{[0,t]}=N_t$. Pruebe que $\Xi$ es una medida de Poisson aleatoria de intensidad $\lambda \times\leb$.
\item Concluya que $N$ es un proceso de Poisson de intensidad $\lambda$. 
\end{enumerate}
\end{ejercicio}
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
\pn---------------------------------------------------\pn
1471
\pn